# Powering On-Prem GenAI: From Infrastructure to Impact
https://www.analyticsvidhya.com/events/datahour/powering-on-prem-genai-from-infrastructure-to-impact/

**Video:** [Powering On-Prem GenAI](https://community.analyticsvidhya.com/c/datahour/powering-on-prem-genai-from-infrastructure-to-impact)

## GenAI Platform

### Core Infrastructure

#### NVIDIA GPUs and CPUs Operator
- Simplifies infrastructure management

#### GitOps with ArgoCD
- Interact with and audit code in repositories

### GetAI Platform - GenAI SDK
- Uses Python libraries
- Integrates with LlamaIndex (wrapper around LlamaIndex)
- Supports LangChain, FastAPI for easy deployment

### GetAI Applications - Self-Service
Workflow:
1. Code
2. Commit
3. Build and test in CI pipeline
4. Deploy via CD pipelines in ArgoCD

### GetAI Platform - Inference Service
- Uses Hugging Face's text generation and embedding inference services
- Models are stored in Hugging Face
- Each time a model is instantiated, a PVC is created to store weights, which are then loaded into the GPU for serving
- A warm model serves the public efficiently

## GenAI Platform - Search Platform

## GenAI Platform - Observability
- **LangFuse** (APM)
- **Elasticsearch** (Search Engine)
- **Prometheus** (Metrics Monitoring)

Understanding how models and LLMs perform and behave allows integration with user feedback.

## GenAI Applications
1. Applied in various domains, with a strong focus on customer support.
2. Case flow:
   - Case enters the right queue
   - Agent picks up and works on the case
   - Agent replies back

**Improvements using GenAI:**
- **Language detection & summarization:** Routing support cases efficiently.
- **Similar case retrieval:** Providing contextual information for new cases.
- **Answer qualification:** Evaluating responses based on:
  - Documentation
  - Completeness
  - Personalization
  - Empathy
  - Clarity
  - Accuracy
  - Formatting
  - Consistency

### Real-time CRM Integration
- Routes cases to the appropriate support team

### Chatbot (Internal Knowledge Base)
- Enables employees to query internal documents
- Supports open-source chat integrations

### Research & Benchmarking
- **DABStep:** Data Agent Benchmark for Multi-Step Reasoning
- References:
  1. Driving operational efficiency at Adyen through LLMs
  2. Accelerating Adyen's support team with smart-ticket routing & AI support agent
  3. AI at Adyen
  4. Elevating code quality with LLM integration
  5. Scaling LLM inference with TGI

## Database Integration in the Real World
- Works with **vector databases** to enhance search relevance.
- A query retrieves the most similar documents and uses them as context for LLM prompts.

## CUDA Setup & Execution
```python
llm.invoke("hello").content
```

- Vector embeddings represent semantic meaning.
- Similar sentences have vectors with close angular proximity.

## Embeddings with LangChain & Hugging Face
```python
from langchain_huggingface import HuggingFaceEmbeddings
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')

sentences = [
    "Our products are shipped worldwide.",
    "Shipping takes around 5 business days.",
    "We partner with DHL for international shipping",
    "Free shipping is available for orders over $50",
    "We are committed to environmental sustainability.",
    "Our company focuses on eco-friendly practices",
    "We use biodegradable packaging materials",
    "Reducing carbon emissions is one of our top priorities"
]

embedded_sentences = np.array(embeddings.embed_documents(sentences))

# Use t-SNE for dimensionality reduction
tsne = TSNE(n_components=2, perplexity=3, random_state=42)
reduced_embeddings = tsne.fit_transform(embedded_sentences)

colors = ['red'] * 4 + ['blue'] * 4

plt.figure(figsize=(10, 10))
for i, (sentence, color) in enumerate(zip(sentences, colors)):
    x, y = reduced_embeddings[i]
    plt.quiver(0, 0, x, y, color=color, angles='xy', scale_units='xy', scale=1, width=0.005)
    plt.text(x + 0.02, y + 0.02, sentence, fontsize=10, bbox=dict(facecolor='white', alpha=0.6))

plt.title('2D Visualization of Document Vectors (t-SNE)', fontsize=16)
plt.xlabel('t-SNE Dimension 1', fontsize=14)
plt.ylabel('t-SNE Dimension 2', fontsize=14)
plt.show()
```

## Chunking with LangChain
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=['\n\n', '\n']
)

chunked_documents = text_splitter.split_documents(documents)

for i, chunk in enumerate(chunked_documents[:3], start=1):
    print(f'Chunk {i} (from {chunk.metadata.get("title", "Unknown")}):\n{chunk.page_content}\n---\n')
```

## FAISS: Vector Search for Chatbots
**FAISS** (Facebook AI Similarity Search) is a high-performance vector search library:
1. Convert document chunks into vector embeddings.
2. Store embeddings in FAISS for fast retrieval.
3. Retrieve relevant document chunks for chatbot queries.

```python
from langchain.vectorstores import FAISS
vectorstore = FAISS.from_documents(chunked_documents, embeddings)

retrieved_docs = vectorstore.similarity_search('chocolate ingredients', k=3)
for i, doc in enumerate(retrieved_docs, start=1):
    print(f'Chunk {i} (from {doc.metadata.get("title", "Unknown")}):\n{doc.page_content}\n---\n')
```

## AI-Powered Chocolate Support Bot
```python
from langchain.prompts import PromptTemplate

prompt_template = PromptTemplate(
    input_variables=['question', 'context'],
    template="""
You are an expert support agent for ChocolatesIsTasty, a premium chocolate company.

A customer has asked:
"{question}"

Use the following internal knowledge if necessary:
{context}

Your response (be concise):
"""
)
```

## Visualizing AI Context Processing
```python
from langchain_core.runnables import Runnable
from IPython.display import display, Markdown

class ProcessDocumentsRunnable(Runnable):
    def invoke(self, docs, config=None, **kwargs):
        md_output = "# Context:\n\n"
        for doc in docs:
            title = doc.metadata.get('title', 'Unknown')
            content = doc.page_content.strip()
            md_output += f'### {title}\n\n{content}\n\n---\n\n'
        display(Markdown(md_output))

        context = "\n\n".join(
            f'Title: {doc.metadata.get("title", "Unknown")}\nContent: {doc.page_content}'
            for doc in docs
        )
        return context
```

This serves as a structured guide for implementing an on-premise GenAI-powered chatbot and inference system.


